{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Airbnb Data Transformation Pipeline\n",
                "\n",
                "This notebook implements a data transformation pipeline for Airbnb listing data using PySpark and SparkNLP."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import time\n",
                "import functools\n",
                "from pyspark.sql import SparkSession\n",
                "import pyspark.sql.functions as F\n",
                "from pyspark.sql.types import IntegerType, DoubleType, StringType\n",
                "import sparknlp\n",
                "from sparknlp.base import *\n",
                "from sparknlp.annotator import *\n",
                "from pyspark.ml import Pipeline\n",
                "\n",
                "# Initialize Spark Session with Spark NLP\n",
                "# Note: ensuring we have a large enough heap for NLP tasks\n",
                "spark = SparkSession.builder \\\n",
                "    .appName(\"AirbnbDataTransformation\") \\\n",
                "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.1.4\") \\\n",
                "    .config(\"spark.driver.memory\", \"4g\") \\\n",
                "    .getOrCreate()\n",
                "\n",
                "spark"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Helper Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def time_execution(func):\n",
                "    @functools.wraps(func)\n",
                "    def wrapper(*args, **kwargs):\n",
                "        start_time = time.time()\n",
                "        result = func(*args, **kwargs)\n",
                "        end_time = time.time()\n",
                "        print(f\"{func.__name__} took {end_time - start_time:.4f} seconds\")\n",
                "        return result\n",
                "    return wrapper"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Data Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "DATA_PATH = \"airbnb.csv\"\n",
                "\n",
                "def load_data(path, sample_fraction=None):\n",
                "    df = spark.read.option(\"header\", \"true\").option(\"multiLine\", \"true\").option(\"escape\", \"\\\"\").csv(path)\n",
                "    if sample_fraction:\n",
                "        df = df.sample(fraction=sample_fraction, seed=42)\n",
                "    return df\n",
                "\n",
                "# Load a sample for development\n",
                "raw_df = load_data(DATA_PATH, sample_fraction=0.01)\n",
                "print(f\"Loaded {raw_df.count()} rows for development.\")\n",
                "raw_df.printSchema()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Transformations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Select and Rename Columns\n",
                "@time_execution\n",
                "def initial_selection(df):\n",
                "    cols_to_keep = {\n",
                "        \"name\": \"name\",\n",
                "        \"price\": \"price\",\n",
                "        \"currency\": \"currency\",\n",
                "        \"reviews\": \"reviews\",\n",
                "        \"ratings\": \"ratings\",\n",
                "        \"location\": \"location\",\n",
                "        \"lat\": \"lat\",\n",
                "        \"long\": \"long\",\n",
                "        \"guests\": \"guests\",\n",
                "        \"description_items\": \"description_items\",\n",
                "        \"category_rating\": \"category_rating\",\n",
                "        \"is_supperhost\": \"is_superhost\",\n",
                "        \"host_number_of_reviews\": \"host_number_of_reviews\",\n",
                "        \"host_rating\": \"host_rating\",\n",
                "        \"hosts_year\": \"host_year\",\n",
                "        \"host_response_rate\": \"host_response_rate\",\n",
                "        \"property_number_of_reviews\": \"property_number_of_reviews\"\n",
                "    }\n",
                "    \n",
                "    # Select only columns that exist in the dataframe to avoid errors if schema slightly differs\n",
                "    existing_columns = df.columns\n",
                "    select_exprs = []\n",
                "    for c, a in cols_to_keep.items():\n",
                "        if c in existing_columns:\n",
                "            select_exprs.append(F.col(c).alias(a))\n",
                "        else:\n",
                "            print(f\"Warning: Column {c} not found in input data.\")\n",
                "            \n",
                "    return df.select(*select_exprs)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Name Transformation: Extract bedrooms, beds, baths\n",
                "@time_execution\n",
                "def transform_name(df):\n",
                "    # Handle Studio: if 'Studio' is in name, treat as 0 bedrooms\n",
                "    # Extract numbers for bedroom, bed, bath using regex\n",
                "    \n",
                "    df = df.withColumn(\"bedrooms\", \n",
                "        F.when(F.lower(F.col(\"name\")).rlike(\"studio\"), 0)\n",
                "         .otherwise(F.regexp_extract(\"name\", r\"(\\d+)\\s+bedroom\", 1).cast(\"int\"))\n",
                "    )\n",
                "    \n",
                "    df = df.withColumn(\"beds\", F.regexp_extract(\"name\", r\"(\\d+)\\s+bed\", 1).cast(\"int\"))\n",
                "    \n",
                "    df = df.withColumn(\"baths\", F.regexp_extract(\"name\", r\"(\\d+(\\.\\d+)?)\\s+bath\", 1).cast(\"double\"))\n",
                "    \n",
                "    return df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Price Transformation: Clean and Normalize\n",
                "@time_execution\n",
                "def transform_price(df):\n",
                "    # Drop null prices\n",
                "    df = df.na.drop(subset=[\"price\"])\n",
                "    \n",
                "    # Clean price string. Remove $, commas, etc.\n",
                "    # Example: \"$1,200.00\" -> \"1200.00\"\n",
                "    df = df.withColumn(\"price_cleaned\", F.regexp_replace(\"price\", \"[^0-9.]\", \"\").cast(\"double\"))\n",
                "    \n",
                "    # For this exercise, we treat 'price_cleaned' as the USD normalized price.\n",
                "    # Enhancements would involve using 'currency' to conversion rates.\n",
                "    \n",
                "    return df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Reviews Transformation: Sentiment Analysis\n",
                "@time_execution\n",
                "def transform_reviews(df):\n",
                "    # Clean reviews column (remove array brackets if string)\n",
                "    # Removing [ ] ' \" chars\n",
                "    df = df.withColumn(\"reviews_clean\", F.regexp_replace(\"reviews\", \"[\\\\[\\\\]'\\\"]\", \"\"))\n",
                "    \n",
                "    # Init SparkNLP Pipeline\n",
                "    # Document Assembler\n",
                "    document = DocumentAssembler() \\\n",
                "        .setInputCol(\"reviews_clean\") \\\n",
                "        .setOutputCol(\"document\")\n",
                "        \n",
                "    # Sentence Detector\n",
                "    # sentence = SentenceDetector() ...\n",
                "    \n",
                "    # Tokenizer\n",
                "    tokenizer = Tokenizer() \\\n",
                "        .setInputCols([\"document\"]) \\\n",
                "        .setOutputCol(\"token\")\n",
                "        \n",
                "    # Sentiment (Using pretrained model 'sentimentdl_use_twitter' or similar if available)\n",
                "    # For standard setup, we use SentimentDLModel.pretrained()\n",
                "    # To ensure it runs without huge downloads loop/failure in this environment, we structure it:\n",
                "    try:\n",
                "        sentiment = SentimentDLModel.pretrained(name=\"sentimentdl_use_twitter\", lang=\"en\") \\\n",
                "            .setInputCols([\"document\", \"token\"]) \\\n",
                "            .setOutputCol(\"sentiment\")\n",
                "            \n",
                "        finisher = Finisher() \\\n",
                "            .setInputCols([\"sentiment\"]) \\\n",
                "            .setOutputCols(\"sentiment_output\")\n",
                "            \n",
                "        pipeline = Pipeline(stages=[document, tokenizer, sentiment, finisher])\n",
                "        \n",
                "        # Fit and Transform\n",
                "        # Since it's a pipeline model, we can just fit on empty or use LightPipeline\n",
                "        model = pipeline.fit(df) # sentiment model is pretrained, so fit is cheap\n",
                "        df = model.transform(df)\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"SparkNLP Sentiment Model failed to load (likely network/size): {e}\")\n",
                "        print(\"Using simplified placeholder for sentiment.\")\n",
                "        df = df.withColumn(\"sentiment_output\", F.lit(\"unknown\"))\n",
                "        \n",
                "    return df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Location Transformation: Extract City\n",
                "@time_execution\n",
                "def transform_location(df):\n",
                "    # City is the first term before the first comma\n",
                "    df = df.withColumn(\"city_extracted\", F.split(F.col(\"location\"), \",\").getItem(0))\n",
                "    return df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Description Items Transformation: Extract and Encode\n",
                "@time_execution\n",
                "def transform_description(df):\n",
                "    # Extract first element of array string\n",
                "    # If format is [\"item1\", \"item2\"], extract item1.\n",
                "    df = df.withColumn(\"desc_first_item\", F.regexp_extract(\"description_items\", r\"['\\\"]([^'\\\"]+)['\\\"]\", 1))\n",
                "    \n",
                "    # Simple encoding: hash the string to a category ID\n",
                "    df = df.withColumn(\"desc_category_id\", F.hash(\"desc_first_item\"))\n",
                "    \n",
                "    return df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Category Rating Transformation\n",
                "@time_execution\n",
                "def transform_category_rating(df):\n",
                "    # Extract columns from 'category_rating'. \n",
                "    # Assumption: format is \"Map(Key -> Value, ...)\" or JSON-like.\n",
                "    # We will attempt to parse simplified assumes: \"amenity=val\"\n",
                "    # For simplicity/robustness without seeing exact data, we just pass through or try to extract 'rating'\n",
                "    # If it mirrors 'Review Scores Rating', we extract that float.\n",
                "    \n",
                "    # Let's extract numeric values if present.\n",
                "    # regex: \"(\\w+)=(\\d+(\\.\\d+)?)\"\n",
                "    return df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Superhost Transformation\n",
                "@time_execution\n",
                "def transform_superhost(df):\n",
                "    # 't'/'f' or 'true'/'false' to 0 and 1\n",
                "    df = df.withColumn(\"is_superhost_binary\", \n",
                "        F.when(F.lower(F.col(\"is_superhost\")).isin(\"t\", \"true\", \"1\"), 1)\n",
                "         .otherwise(0)\n",
                "    )\n",
                "    return df"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Pipeline Execution"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def apply_pipeline(df):\n",
                "    # Apply transformations sequentially\n",
                "    df = initial_selection(df)\n",
                "    df = transform_name(df)\n",
                "    df = transform_price(df)\n",
                "    df = transform_reviews(df)\n",
                "    df = transform_location(df)\n",
                "    df = transform_description(df)\n",
                "    df = transform_category_rating(df)\n",
                "    df = transform_superhost(df)\n",
                "    return df\n",
                "\n",
                "# Split Data\n",
                "train_data, val_data, test_data = raw_df.randomSplit([0.7, 0.15, 0.15], seed=42)\n",
                "\n",
                "# Apply Transformations\n",
                "print(\"Transforming Training Data...\")\n",
                "train_transformed = apply_pipeline(train_data)\n",
                "\n",
                "print(\"Transforming Validation Data...\")\n",
                "val_transformed = apply_pipeline(val_data)\n",
                "\n",
                "print(\"Transforming Test Data...\")\n",
                "test_transformed = apply_pipeline(test_data)\n",
                "\n",
                "# Show Sample\n",
                "train_transformed.show(5)\n",
                "train_transformed.printSchema()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}