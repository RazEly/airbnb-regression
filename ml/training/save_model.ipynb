{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80885d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model Saver Module\n",
    "\n",
    "Saves all trained ML pipeline components for deployment:\n",
    "- PySpark ML models (GBT, Imputers, Scaler)\n",
    "- VectorAssemblers (serialized)\n",
    "- Lookup dictionaries (JSON)\n",
    "- Cluster data for KNN\n",
    "- Metadata\n",
    "\n",
    "Author: ML Pipeline Team\n",
    "Date: 2026-01-19\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c78ef6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf3ad59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15578f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pipeline_artifacts(\n",
    "    gbt_model,\n",
    "    imputer_continuous_model,\n",
    "    imputer_binary_model,\n",
    "    scaler_model,\n",
    "    assembler_continuous,\n",
    "    assembler_binary,\n",
    "    assembler_final,\n",
    "    train_df,\n",
    "    city_medians_dict,\n",
    "    city_centers_dict,\n",
    "    global_median,\n",
    "    continuous_features,\n",
    "    binary_features,\n",
    "    performance_metrics,\n",
    "    output_dir=\"./models\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Save all pipeline components to disk.\n",
    "\n",
    "    Creates directory structure:\n",
    "    models/\n",
    "    ├── gbt_model/               (PySpark GBTRegressionModel)\n",
    "    ├── imputer_continuous/      (PySpark ImputerModel)\n",
    "    ├── imputer_binary/          (PySpark ImputerModel)\n",
    "    ├── scaler_continuous/       (PySpark StandardScalerModel)\n",
    "    ├── assembler_configs.json   (VectorAssembler configurations)\n",
    "    ├── city_medians.json\n",
    "    ├── city_centers.json        (NEW: City center coordinates)\n",
    "    ├── global_median.json\n",
    "    ├── cluster_medians.json\n",
    "    ├── cluster_data.json\n",
    "    ├── top_cities.json\n",
    "    └── metadata.json\n",
    "\n",
    "    Args:\n",
    "        gbt_model: Trained GBTRegressionModel\n",
    "        imputer_continuous_model: ImputerModel for continuous features\n",
    "        imputer_binary_model: ImputerModel for binary features\n",
    "        scaler_model: StandardScalerModel\n",
    "        assembler_continuous: VectorAssembler for continuous features\n",
    "        assembler_binary: VectorAssembler for binary features\n",
    "        assembler_final: Final VectorAssembler\n",
    "        train_df: Training DataFrame (for cluster extraction)\n",
    "        city_medians_dict: Dict of city -> median price (log-scale)\n",
    "        city_centers_dict: Dict of city -> {\"lat\": x, \"lon\": y} (median coordinates)\n",
    "        global_median: Global median price (log-scale)\n",
    "        continuous_features: List of continuous feature names\n",
    "        binary_features: List of binary feature names\n",
    "        performance_metrics: Dict with model performance metrics\n",
    "        output_dir: Directory to save all artifacts\n",
    "    \"\"\"\n",
    "\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"SAVING PIPELINE ARTIFACTS TO: {output_dir}\")\n",
    "    print(f\"{'=' * 70}\\n\")\n",
    "\n",
    "    # Helper function to convert /dbfs/ → dbfs:/ for PySpark writes in Databricks\n",
    "    def get_spark_path(base_dir, subdir):\n",
    "        \"\"\"\n",
    "        Convert /dbfs/ path to dbfs:/ for PySpark writes in Databricks.\n",
    "\n",
    "        Args:\n",
    "            base_dir: Output directory (e.g., \"/dbfs/FileStore/models/production\" or \"./models\")\n",
    "            subdir: Subdirectory name (e.g., \"gbt_model\")\n",
    "\n",
    "        Returns:\n",
    "            Properly formatted path for PySpark .save() operations\n",
    "            - Databricks: \"/dbfs/...\" → \"dbfs:/...\"\n",
    "            - Local: \"./models/...\" → \"./models/...\"\n",
    "        \"\"\"\n",
    "        if base_dir.startswith(\"/dbfs/\"):\n",
    "            return base_dir.replace(\"/dbfs/\", \"dbfs:/\") + f\"/{subdir}\"\n",
    "        else:\n",
    "            return f\"{base_dir}/{subdir}\"\n",
    "\n",
    "    # 1. Save PySpark models\n",
    "    print(\"[1/9] Saving PySpark models...\")\n",
    "    gbt_model.write().overwrite().save(get_spark_path(output_dir, \"gbt_model\"))\n",
    "    print(\"  ✓ GBT model saved\")\n",
    "\n",
    "    imputer_continuous_model.write().overwrite().save(\n",
    "        get_spark_path(output_dir, \"imputer_continuous\")\n",
    "    )\n",
    "    print(\"  ✓ Continuous imputer saved\")\n",
    "\n",
    "    imputer_binary_model.write().overwrite().save(\n",
    "        get_spark_path(output_dir, \"imputer_binary\")\n",
    "    )\n",
    "    print(\"  ✓ Binary imputer saved\")\n",
    "\n",
    "    scaler_model.write().overwrite().save(\n",
    "        get_spark_path(output_dir, \"scaler_continuous\")\n",
    "    )\n",
    "    print(\"  ✓ Scaler saved\")\n",
    "\n",
    "    # 2. Save VectorAssembler configurations (column names only)\n",
    "    print(\"\\n[2/9] Saving VectorAssembler configurations...\")\n",
    "\n",
    "    # PySpark VectorAssemblers can't be pickled, so save their configurations\n",
    "    assembler_configs = {\n",
    "        \"assembler_continuous\": {\n",
    "            \"input_cols\": assembler_continuous.getInputCols(),\n",
    "            \"output_col\": assembler_continuous.getOutputCol(),\n",
    "        },\n",
    "        \"assembler_binary\": {\n",
    "            \"input_cols\": assembler_binary.getInputCols(),\n",
    "            \"output_col\": assembler_binary.getOutputCol(),\n",
    "        },\n",
    "        \"assembler_final\": {\n",
    "            \"input_cols\": assembler_final.getInputCols(),\n",
    "            \"output_col\": assembler_final.getOutputCol(),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    with open(f\"{output_dir}/assembler_configs.json\", \"w\") as f:\n",
    "        json.dump(assembler_configs, f, indent=2)\n",
    "\n",
    "    print(\"  ✓ Assembler configurations saved\")\n",
    "\n",
    "    # 3. Save city medians\n",
    "    print(\"\\n[3/9] Saving city medians...\")\n",
    "    with open(f\"{output_dir}/city_medians.json\", \"w\") as f:\n",
    "        json.dump(city_medians_dict, f, indent=2)\n",
    "    print(f\"  ✓ Saved {len(city_medians_dict)} city medians\")\n",
    "\n",
    "    # 4. Save city centers (NEW: median coordinates per city)\n",
    "    print(\"\\n[4/9] Saving city centers...\")\n",
    "    with open(f\"{output_dir}/city_centers.json\", \"w\") as f:\n",
    "        json.dump(city_centers_dict, f, indent=2)\n",
    "    print(f\"  ✓ Saved {len(city_centers_dict)} city center coordinates\")\n",
    "\n",
    "    # 5. Save global median\n",
    "    print(\"\\n[5/9] Saving global median...\")\n",
    "    with open(f\"{output_dir}/global_median.json\", \"w\") as f:\n",
    "        json.dump({\"global_median\": float(global_median)}, f, indent=2)\n",
    "    print(f\"  ✓ Global median: {global_median:.4f}\")\n",
    "\n",
    "    # 6. Save cluster data as Parquet\n",
    "    print(\"\\n[6/9] Saving cluster data as Parquet...\")\n",
    "    num_points = save_cluster_data_parquet(train_df, output_dir)\n",
    "    print(f\"  ✓ Saved {num_points:,} cluster points as Parquet\")\n",
    "\n",
    "    # 7. Extract and save cluster medians\n",
    "    print(\"\\n[7/9] Extracting and saving cluster medians...\")\n",
    "    cluster_medians = extract_cluster_medians(train_df)\n",
    "    with open(f\"{output_dir}/cluster_medians.json\", \"w\") as f:\n",
    "        json.dump(cluster_medians, f, indent=2)\n",
    "    print(f\"  ✓ Saved {len(cluster_medians)} cluster medians\")\n",
    "\n",
    "    # 8. Extract top cities\n",
    "    print(\"\\n[8/9] Extracting top cities...\")\n",
    "    top_cities = list(city_medians_dict.keys())\n",
    "    with open(f\"{output_dir}/top_cities.json\", \"w\") as f:\n",
    "        json.dump(top_cities, f, indent=2)\n",
    "    print(f\"  ✓ Saved {len(top_cities)} top cities\")\n",
    "\n",
    "    # 9. Save metadata\n",
    "    print(\"\\n[9/9] Saving metadata...\")\n",
    "    metadata = {\n",
    "        \"training_date\": datetime.now().isoformat(),\n",
    "        \"continuous_features\": continuous_features,\n",
    "        \"binary_features\": binary_features,\n",
    "        \"performance_metrics\": performance_metrics,\n",
    "        \"num_cities\": len(city_medians_dict),\n",
    "        \"num_clusters\": len(cluster_medians),\n",
    "        \"global_median\": float(global_median),\n",
    "    }\n",
    "    with open(f\"{output_dir}/metadata.json\", \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    print(\"  ✓ Metadata saved\")\n",
    "\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"✓ ALL ARTIFACTS SAVED SUCCESSFULLY\")\n",
    "    print(f\"{'=' * 70}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ae8f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cluster_data_parquet(train_df, output_dir):\n",
    "    \"\"\"\n",
    "    Save cluster training points as Parquet for KNN assignment.\n",
    "\n",
    "    Uses Parquet format instead of JSON for 80-85% size reduction\n",
    "    (3.1MB JSON → 400-600KB Parquet) while maintaining 100% accuracy.\n",
    "\n",
    "    Args:\n",
    "        train_df: Training DataFrame with cluster_id, city, lat, long columns\n",
    "        output_dir: Directory to save Parquet file\n",
    "\n",
    "    Returns:\n",
    "        Number of cluster points saved\n",
    "    \"\"\"\n",
    "    print(\"  - Saving cluster points as Parquet...\")\n",
    "\n",
    "    # Select and cast columns explicitly for schema consistency\n",
    "    cluster_df = train_df.filter(F.col(\"cluster_id\") != -1).select(\n",
    "        F.col(\"city\").cast(\"string\"),\n",
    "        F.col(\"lat\").cast(\"double\"),\n",
    "        F.col(\"long\").cast(\"double\"),\n",
    "        F.col(\"cluster_id\").cast(\"long\"),\n",
    "    )\n",
    "\n",
    "    # Determine if running on Databricks or local\n",
    "    # Databricks: output_dir = \"/dbfs/...\" → use \"dbfs:/...\" for Spark writes\n",
    "    # Local: output_dir = \"./models/...\" → use as-is\n",
    "    if output_dir.startswith(\"/dbfs/\"):\n",
    "        output_path = output_dir.replace(\"/dbfs/\", \"dbfs:/\") + \"/cluster_data.parquet\"\n",
    "    else:\n",
    "        output_path = f\"{output_dir}/cluster_data.parquet\"\n",
    "\n",
    "    # Save as Parquet with snappy compression\n",
    "    cluster_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "    row_count = cluster_df.count()\n",
    "    print(f\"  - Saved {row_count:,} cluster points to {output_path}\")\n",
    "\n",
    "    return row_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e855505e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cluster_medians(train_df):\n",
    "    \"\"\"\n",
    "    Extract cluster medians from training data.\n",
    "\n",
    "    Args:\n",
    "        train_df: Training DataFrame with cluster_id, city, price_cleaned columns\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"Greater London|42\": 4.8234,  # log-transformed median\n",
    "            \"Greater London|15\": 4.9512,\n",
    "            ...\n",
    "        }\n",
    "    \"\"\"\n",
    "    print(\"  - Computing cluster medians...\")\n",
    "\n",
    "    cluster_median_rows = (\n",
    "        train_df.filter(F.col(\"cluster_id\") != -1)\n",
    "        .groupBy(\"city\", \"cluster_id\")\n",
    "        .agg(F.expr(\"percentile_approx(price_cleaned, 0.5)\").alias(\"median\"))\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "    cluster_medians = {}\n",
    "    for row in cluster_median_rows:\n",
    "        city = row[\"city\"]\n",
    "        cluster_id = int(row[\"cluster_id\"])\n",
    "        median = float(row[\"median\"])\n",
    "        key = f\"{city}|{cluster_id}\"\n",
    "        cluster_medians[key] = median\n",
    "\n",
    "    print(f\"  - Computed medians for {len(cluster_medians)} clusters\")\n",
    "    return cluster_medians"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
