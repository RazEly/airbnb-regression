{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a5d7194",
   "metadata": {},
   "source": [
    "Databricks notebook source\n",
    "MAGIC %md\n",
    "MAGIC ## Imports and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af646e43",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3584693",
   "metadata": {},
   "source": [
    "MAGIC %pip install hdbscan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844537d9",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae502113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4806594",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b61e5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "import seaborn as sns\n",
    "from hdbscan import HDBSCAN\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import (\n",
    "    Imputer,\n",
    "    OneHotEncoder,\n",
    "    StandardScaler,\n",
    "    StringIndexer,\n",
    "    VectorAssembler,\n",
    ")\n",
    "from pyspark.ml.regression import GBTRegressor, LinearRegression, RandomForestRegressor\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType\n",
    "from scipy import stats\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d86ef8",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa6d40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_account = \"lab94290\"\n",
    "container = \"airbnb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab5c46d",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7f26f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sas_token = \"sp=rle&st=2025-12-24T17:37:04Z&se=2026-02-28T01:52:04Z&spr=https&sv=2024-11-04&sr=c&sig=a0lx%2BS6PuS%2FvJ9Tbt4NKdCJHLE9d1Y1D6vpE1WKFQtk%3D\"\n",
    "sas_token = sas_token.lstrip(\"?\")\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"SAS\"\n",
    ")\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.sas.token.provider.type.{storage_account}.dfs.core.windows.net\",\n",
    "    \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\",\n",
    ")\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.sas.fixed.token.{storage_account}.dfs.core.windows.net\", sas_token\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804a1b0d",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce993aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/airbnb_1_12_parquet\"\n",
    "print(path)\n",
    "airbnb = spark.read.parquet(path)\n",
    "calendar = spark.read.parquet(\"/airbnb_calendar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae46843",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5755ce17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark settings\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d03e4df",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00f8e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBTITLE 1,Untitled\n",
    "# Create a temporary directory that will be auto-cleaned\n",
    "OUTPUT_DIR = \"/dbfs/FileStore/models/production\"\n",
    "dbutils.fs.mkdirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ad6751",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc6ffb4",
   "metadata": {},
   "source": [
    "MAGIC %md\n",
    "MAGIC ## Calendar Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7e3845",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ca2aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea799d00",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f5dcaf",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "from pyspark.sql import DataFrame, SparkSession, Window\n",
    "from pyspark.sql.functions import (\n",
    "    avg,\n",
    "    col,\n",
    "    count,\n",
    "    lit,\n",
    "    log1p,\n",
    "    regexp_replace,\n",
    "    to_date,\n",
    "    when,\n",
    ")\n",
    "from pyspark.sql.types import FloatType, StringType, StructField, StructType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43994347",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906c561c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_calendar(calendar: DataFrame):\n",
    "    window = Window.partitionBy(\"city\")\n",
    "    calendar = (\n",
    "        calendar.select(\"adjusted_price\", \"city\", \"date\")\n",
    "        .withColumn(\n",
    "            \"price_adjusted_clean\",\n",
    "            F.regexp_replace(F.col(\"adjusted_price\"), \"[$,]\", \"\").cast(\"double\"),\n",
    "        )\n",
    "        .filter(F.col(\"price_adjusted_clean\").isNotNull())\n",
    "        .withColumn(\"price\", F.log1p(F.col(\"price_adjusted_clean\")))\n",
    "        .withColumn(\"date\", F.to_date(F.col(\"date\"), \"yyyy-MM-dd\"))\n",
    "        .withColumn(\"city\", F.lower(F.regexp_replace(F.col(\"city\"), \"_\", \" \")))\n",
    "        .withColumn(\"day_month\", F.date_format(F.col(\"date\"), \"MM-dd\"))\n",
    "        .groupBy(\"city\", \"day_month\")\n",
    "        .agg(F.avg(\"price\").alias(\"price\"), F.count(F.lit(1)).alias(\"count\"))\n",
    "        .orderBy(\"city\", \"day_month\")\n",
    "        .filter(F.col(\"count\") > 10)\n",
    "        .withColumn(\n",
    "            \"base_price\",\n",
    "            F.avg(F.when(F.col(\"day_month\") == \"09-25\", F.col(\"price\"))).over(window),\n",
    "        )\n",
    "        .withColumn(\"price_relative\", F.col(\"price\") - F.col(\"base_price\"))\n",
    "        .withColumn(\n",
    "            \"price_normalized\",\n",
    "            (F.col(\"price\") - F.avg(\"price\").over(window))\n",
    "            / F.stddev(\"price\").over(window),\n",
    "        )\n",
    "        .withColumn(\"num_days\", F.count(F.lit(1)).over(window))\n",
    "        .filter(F.col(\"num_days\") >= 365)\n",
    "        .drop(\"base_price\", \"num_days\", \"count\", \"price\")\n",
    "        .withColumnRenamed(\"day_month\", \"date\")\n",
    "    )\n",
    "\n",
    "    # QuantileDiscretizer for stoplight\n",
    "    discretizer = QuantileDiscretizer(\n",
    "        numBuckets=4,\n",
    "        inputCol=\"price_normalized\",\n",
    "        outputCol=\"stoplight_bucket\",\n",
    "        handleInvalid=\"skip\",\n",
    "    )\n",
    "    calendar = discretizer.fit(calendar).transform(calendar)\n",
    "\n",
    "    calendar = calendar.withColumn(\n",
    "        \"stoplight\",\n",
    "        F.when(F.col(\"stoplight_bucket\") == 0, F.lit(\"green\"))\n",
    "        .when(F.col(\"stoplight_bucket\") == 1, F.lit(\"yellow\"))\n",
    "        .when(F.col(\"stoplight_bucket\") == 2, F.lit(\"orange\"))\n",
    "        .when(F.col(\"stoplight_bucket\") == 3, F.lit(\"red\"))\n",
    "        .otherwise(F.lit(None)),\n",
    "    ).drop(\"stoplight_bucket\", \"price_normalized\")\n",
    "    return calendar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8131d5",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1762b839",
   "metadata": {},
   "source": [
    "MAGIC %md\n",
    "MAGIC ## Data Ingestion & Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cf0506",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc37e5f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def initial_selection(df):\n",
    "    # Define columns to keep. For most, the name remains the same.\n",
    "    # For a few, a rename (alias) is explicitly handled to match subsequent transformations.\n",
    "    columns_to_select = [\n",
    "        \"name\",\n",
    "        \"price\",\n",
    "        \"pricing_details\",\n",
    "        \"currency\",\n",
    "        \"reviews\",\n",
    "        \"ratings\",\n",
    "        \"location\",\n",
    "        \"lat\",\n",
    "        \"long\",\n",
    "        \"guests\",\n",
    "        \"category_rating\",\n",
    "        \"host_number_of_reviews\",\n",
    "        \"host_rating\",\n",
    "        \"host_response_rate\",\n",
    "        \"property_number_of_reviews\",\n",
    "        \"details\",\n",
    "        \"amenities\",\n",
    "        \"description\",\n",
    "        \"location_details\",\n",
    "    ]\n",
    "\n",
    "    # Special cases where original column name is different from the desired final name\n",
    "    # (e.g., handling typos or inconsistencies from the raw data)\n",
    "    renamed_columns_map = {\n",
    "        \"is_supperhost\": \"is_superhost\",  # Original might be 'is_supperhost', desired is 'is_superhost'\n",
    "        \"hosts_year\": \"host_year\",  # Original might be 'hosts_year', desired is 'host_year'\n",
    "    }\n",
    "\n",
    "    existing_columns = df.columns\n",
    "    select_exprs = []\n",
    "\n",
    "    # Add columns that are not renamed\n",
    "    for col_name in columns_to_select:\n",
    "        if isinstance(col_name, str):\n",
    "            if col_name in existing_columns:\n",
    "                select_exprs.append(F.col(col_name))\n",
    "            else:\n",
    "                print(\n",
    "                    f\"Warning: Column '{col_name}' not found in input data and will be dropped.\"\n",
    "                )\n",
    "        else:\n",
    "            # It's a Column object (expression)\n",
    "            select_exprs.append(col_name)\n",
    "\n",
    "    # Add columns that are renamed\n",
    "    for old_name, new_name in renamed_columns_map.items():\n",
    "        if old_name in existing_columns:\n",
    "            select_exprs.append(F.col(old_name).alias(new_name))\n",
    "        elif (\n",
    "            new_name in existing_columns and old_name != new_name\n",
    "        ):  # If new_name already exists, don't rename from old_name\n",
    "            select_exprs.append(F.col(new_name))\n",
    "        else:\n",
    "            print(\n",
    "                f\"Warning: Column '{old_name}' (to be renamed to '{new_name}') not found in input data. It will be dropped.\"\n",
    "            )\n",
    "\n",
    "    if not select_exprs:\n",
    "        raise ValueError(\"No columns found to select after initial_selection.\")\n",
    "\n",
    "    return df.select(*select_exprs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d671fb2",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46729ecf",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Set Schema\n",
    "def set_schema(df):\n",
    "    from pyspark.sql.types import (\n",
    "        ArrayType,\n",
    "        BooleanType,\n",
    "        DoubleType,\n",
    "        StringType,\n",
    "        StructField,\n",
    "        StructType,\n",
    "    )\n",
    "\n",
    "    # Define the schema mapping based on inspection of airbnb.csv\n",
    "    dtype_mapping = {\n",
    "        \"name\": StringType(),\n",
    "        \"price\": DoubleType(),\n",
    "        \"currency\": StringType(),\n",
    "        \"reviews\": ArrayType(StringType()),\n",
    "        \"ratings\": DoubleType(),\n",
    "        \"location\": StringType(),\n",
    "        \"lat\": DoubleType(),\n",
    "        \"long\": DoubleType(),\n",
    "        \"guests\": DoubleType(),\n",
    "        \"description_items\": StringType(),\n",
    "        \"pricing_details\": StructType(\n",
    "            [\n",
    "                StructField(\"airbnb_service_fee\", DoubleType(), True),\n",
    "                StructField(\"cleaning_fee\", DoubleType(), True),\n",
    "                StructField(\"initial_price_per_night\", DoubleType(), True),\n",
    "                StructField(\"num_of_nights\", DoubleType(), True),\n",
    "                StructField(\"price_per_night\", DoubleType(), True),\n",
    "                StructField(\"price_without_fees\", DoubleType(), True),\n",
    "                StructField(\"special_offer\", StringType(), True),\n",
    "                StructField(\"taxes\", StringType(), True),\n",
    "            ]\n",
    "        ),\n",
    "        \"category_rating\": ArrayType(\n",
    "            StructType(\n",
    "                [\n",
    "                    StructField(\"name\", StringType(), True),\n",
    "                    StructField(\"value\", StringType(), True),\n",
    "                ]\n",
    "            )\n",
    "        ),\n",
    "        \"host_number_of_reviews\": DoubleType(),\n",
    "        \"host_rating\": DoubleType(),\n",
    "        \"host_response_rate\": StringType(),\n",
    "        \"property_number_of_reviews\": DoubleType(),\n",
    "        \"is_superhost\": BooleanType(),\n",
    "        \"host_year\": DoubleType(),\n",
    "        \"details\": ArrayType(StringType()),\n",
    "        \"description\": StringType(),\n",
    "        \"location_details\": StringType(),\n",
    "    }\n",
    "\n",
    "    json_parse_columns = [\"reviews\", \"details\", \"category_rating\", \"pricing_details\"]\n",
    "\n",
    "    for col_name, data_type in dtype_mapping.items():\n",
    "        if col_name in df.columns:\n",
    "            if col_name in json_parse_columns:\n",
    "                # Use from_json to parse stringified JSON arrays/structs\n",
    "                df = df.withColumn(col_name, F.from_json(F.col(col_name), data_type))\n",
    "            else:\n",
    "                # Use cast for simple types\n",
    "                # Using try_cast to handle malformed inputs (e.g. 'null' string)\n",
    "                df = df.withColumn(\n",
    "                    col_name,\n",
    "                    F.expr(f\"try_cast({col_name} as {data_type.simpleString()})\"),\n",
    "                )\n",
    "        else:\n",
    "            print(f\"Warning: Column {col_name} not found for schema enforcement.\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801192bc",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03a5853",
   "metadata": {},
   "source": [
    "MAGIC %md\n",
    "MAGIC ## Core Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328624d8",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb94a47",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "def transform_name(df):\n",
    "    df_temp = df.withColumn(\n",
    "        \"is_studio_binary\",\n",
    "        F.when(F.lower(F.col(\"name\")).contains(\"studio\"), 1).otherwise(0),\n",
    "    )\n",
    "    return df_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea2c811",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515d3bf7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def transform_details(df):\n",
    "    re_beds = r\"(?i)\\b(\\d+\\.?\\d*)\\s+beds?\\b\"\n",
    "    re_bedrooms = r\"(?i)\\b(\\d+\\.?\\d*)\\s+bedrooms?\\b\"\n",
    "\n",
    "    # This pattern captures the number ONLY if it is NOT preceded by the word \"shared\"\n",
    "    re_baths = r\"(?i)(?<!shared\\s)\\b(\\d+\\.?\\d*)\\s+bath(?:s|rooms?)?\\b\"\n",
    "\n",
    "    # 2. Create the search string\n",
    "    df = df.withColumn(\"details_str\", F.concat_ws(\" \", F.col(\"details\")))\n",
    "\n",
    "    # 3. Extraction Logic\n",
    "    for col_name, pattern in [\n",
    "        (\"num_beds\", re_beds),\n",
    "        (\"num_bedrooms\", re_bedrooms),\n",
    "        (\"num_baths\", re_baths),\n",
    "    ]:\n",
    "        ext_details = F.regexp_extract(\"details_str\", pattern, 1)\n",
    "        ext_name = F.regexp_extract(\"name\", pattern, 1)\n",
    "\n",
    "        df = df.withColumn(\n",
    "            col_name,\n",
    "            F.coalesce(\n",
    "                F.nullif(ext_details, F.lit(\"\")),\n",
    "                F.nullif(ext_name, F.lit(\"\")),\n",
    "                F.lit(\"0\"),\n",
    "            ).cast(\"float\"),\n",
    "        )\n",
    "\n",
    "    return df.drop(\"details_str\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d02db5e",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41239da9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def filter_top_k_cities(df, k=100):\n",
    "    city_counts = (\n",
    "        df.groupBy(\"city\").count().orderBy(F.desc(\"count\")).limit(k).select(\"city\")\n",
    "    )\n",
    "    top_cities = [row[\"city\"] for row in city_counts.collect()]\n",
    "    return df.filter(F.col(\"city\").isin(top_cities))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcfc27a",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c910d169",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def prepare_price(df):\n",
    "    \"\"\"\n",
    "    Prepares price column without filtering nulls.\n",
    "    Creates log-transformed price_cleaned column but keeps all rows.\n",
    "    Should be called early in pipeline, before clustering.\n",
    "\n",
    "    This allows clustering to use full geographic density (all listings with coordinates)\n",
    "    while still having price_cleaned available for later use.\n",
    "    \"\"\"\n",
    "    df = df.filter(F.col(\"pricing_details.num_of_nights\") < 30)\n",
    "\n",
    "    # Use price_per_night from pricing_details if available\n",
    "    df = df.withColumn(\n",
    "        \"price\",\n",
    "        F.when(\n",
    "            F.col(\"pricing_details.price_per_night\").isNotNull(),\n",
    "            F.col(\"pricing_details.price_per_night\"),\n",
    "        ).otherwise(F.col(\"price\")),\n",
    "    )\n",
    "\n",
    "    # Create log-transformed column (will be null where price is null/invalid)\n",
    "    df = df.withColumn(\n",
    "        \"price_cleaned\",\n",
    "        F.when(F.col(\"price\") > 0, F.log1p(F.col(\"price\"))).otherwise(None),\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe644d0",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc99e2e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def filter_valid_prices(df):\n",
    "    \"\"\"\n",
    "    Filters out rows with null or invalid prices.\n",
    "    Should be called after clustering is complete to preserve geographic density.\n",
    "    \"\"\"\n",
    "    return df.filter(\n",
    "        (F.col(\"price\").isNotNull())\n",
    "        & (F.col(\"price\") > 0)\n",
    "        & (F.col(\"price_cleaned\").isNotNull())\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eebc0a6",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1531130",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def transform_location(df):\n",
    "    # City is the first term before the first comma\n",
    "    df = df.withColumn(\"city\", F.split(F.col(\"location\"), \",\").getItem(0))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048f889f",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9427b575",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Superhost Transformation\n",
    "def transform_superhost(df):\n",
    "    # 't'/'f' or 'true'/'false' to 0 and 1\n",
    "    df = df.withColumn(\n",
    "        \"is_superhost_binary\",\n",
    "        F.when(F.lower(F.col(\"is_superhost\")).isin(\"t\", \"true\", \"1\"), 1).otherwise(0),\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4640524",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544f90f1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Interaction Features\n",
    "def create_interaction_features(df):\n",
    "    \"\"\"\n",
    "    Creates interaction features based on domain knowledge.\n",
    "\n",
    "    Capacity-related interactions:\n",
    "    - How many beds/bedrooms per guest\n",
    "    - Guest to bedroom ratio\n",
    "\n",
    "    Quality indicators:\n",
    "    - Superhost combined with rating\n",
    "    - Review volume combined with quality\n",
    "\n",
    "    Space metrics:\n",
    "    - Total room count\n",
    "    - Bed density in bedrooms\n",
    "    \"\"\"\n",
    "\n",
    "    # Optimized: Combine all withColumn operations into single select for better performance\n",
    "    # This reduces the number of passes over the data from 9 to 1\n",
    "    return df.select(\n",
    "        \"*\",\n",
    "        # Capacity-related interactions\n",
    "        # Add 1 to denominators to avoid division by zero\n",
    "        (F.col(\"num_beds\") / (F.coalesce(F.col(\"guests\"), F.lit(0)) + 1)).alias(\n",
    "            \"beds_per_guest\"\n",
    "        ),\n",
    "        (F.col(\"num_bedrooms\") / (F.coalesce(F.col(\"guests\"), F.lit(0)) + 1)).alias(\n",
    "            \"bedrooms_per_guest\"\n",
    "        ),\n",
    "        (\n",
    "            F.coalesce(F.col(\"guests\"), F.lit(0))\n",
    "            / (F.coalesce(F.col(\"num_bedrooms\"), F.lit(0)) + 1)\n",
    "        ).alias(\"guest_capacity_ratio\"),\n",
    "        # Quality indicators\n",
    "        (\n",
    "            F.coalesce(F.col(\"is_superhost_binary\"), F.lit(0))\n",
    "            * F.coalesce(F.col(\"host_rating\"), F.lit(0))\n",
    "        ).alias(\"superhost_rating_interaction\"),\n",
    "        (\n",
    "            F.coalesce(F.col(\"property_number_of_reviews\"), F.lit(0))\n",
    "            * F.coalesce(F.col(\"ratings\"), F.lit(0))\n",
    "        ).alias(\"review_volume_quality\"),\n",
    "        # Space metrics\n",
    "        (\n",
    "            F.coalesce(F.col(\"num_bedrooms\"), F.lit(0))\n",
    "            + F.coalesce(F.col(\"num_baths\"), F.lit(0))\n",
    "        ).alias(\"total_rooms\"),\n",
    "        (F.col(\"num_beds\") / (F.coalesce(F.col(\"num_bedrooms\"), F.lit(0)) + 1)).alias(\n",
    "            \"bed_to_bedroom_ratio\"\n",
    "        ),\n",
    "        (\n",
    "            (\n",
    "                F.coalesce(F.col(\"num_bedrooms\"), F.lit(0))\n",
    "                + F.coalesce(F.col(\"num_baths\"), F.lit(0))\n",
    "            )\n",
    "            / (F.coalesce(F.col(\"guests\"), F.lit(0)) + 1)\n",
    "        ).alias(\"rooms_per_guest\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f540020c",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d3d44d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def transform_amenities(df):\n",
    "    from pyspark.sql.types import ArrayType, StringType, StructField, StructType\n",
    "\n",
    "    # Define Schema\n",
    "    item_schema = StructType(\n",
    "        [\n",
    "            StructField(\"name\", StringType(), True),\n",
    "            StructField(\"value\", StringType(), True),\n",
    "        ]\n",
    "    )\n",
    "    group_schema = StructType(\n",
    "        [\n",
    "            StructField(\"group_name\", StringType(), True),\n",
    "            StructField(\"items\", ArrayType(item_schema), True),\n",
    "        ]\n",
    "    )\n",
    "    amenities_schema = ArrayType(group_schema)\n",
    "\n",
    "    # Parse JSON\n",
    "    df = df.withColumn(\n",
    "        \"amenities_parsed\", F.from_json(F.col(\"amenities\"), amenities_schema)\n",
    "    )\n",
    "\n",
    "    # Calculate count excluding \"Not included\"\n",
    "    # Using higher-order functions:\n",
    "    # 1. Filter groups where group_name != 'Not included'\n",
    "    # 2. Transform the filtered groups to get the size of their 'items' array\n",
    "    # 3. Aggregate (sum) the sizes\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"amenities_count\",\n",
    "        F.expr(\n",
    "            \"\"\"\n",
    "            aggregate(\n",
    "                transform(\n",
    "                    filter(amenities_parsed, x -> x.group_name != 'Not included'),\n",
    "                    x -> size(x.items)\n",
    "                ),\n",
    "                0,\n",
    "                (acc, x) -> acc + x\n",
    "            )\n",
    "        \"\"\"\n",
    "        ).cast(\"integer\"),\n",
    "    )\n",
    "\n",
    "    # Fill nulls with 0\n",
    "    df = df.fillna(0, subset=[\"amenities_count\"])\n",
    "\n",
    "    return df.drop(\"amenities\", \"amenities_parsed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf80d7f",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ec349f",
   "metadata": {},
   "source": [
    "MAGIC %md\n",
    "MAGIC ## City-Level Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f108480",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78386c5d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def fit_transform_city(train_df, val_df):\n",
    "    # Compute median price per city in train_df\n",
    "    city_medians = train_df.groupBy(\"city\").agg(\n",
    "        F.percentile_approx(\"price_cleaned\", 0.5).alias(\"median_city\")\n",
    "    )\n",
    "\n",
    "    # Compute median coordinates (city centers) per city\n",
    "    city_centers = train_df.groupBy(\"city\").agg(\n",
    "        F.percentile_approx(\"lat\", 0.5).alias(\"center_lat\"),\n",
    "        F.percentile_approx(\"long\", 0.5).alias(\"center_lon\"),\n",
    "    )\n",
    "\n",
    "    # Compute global median price from train_df\n",
    "    global_median = train_df.agg(F.percentile_approx(\"price_cleaned\", 0.5)).first()[0]\n",
    "\n",
    "    # Save state for later use (as dicts)\n",
    "    city_medians_dict = {\n",
    "        row[\"city\"]: row[\"median_city\"] for row in city_medians.collect()\n",
    "    }\n",
    "\n",
    "    # Save city centers as dict\n",
    "    city_centers_dict = {\n",
    "        row[\"city\"]: {\"lat\": float(row[\"center_lat\"]), \"lon\": float(row[\"center_lon\"])}\n",
    "        for row in city_centers.collect()\n",
    "        if row[\"center_lat\"] is not None and row[\"center_lon\"] is not None\n",
    "    }\n",
    "\n",
    "    # Add median_city column to train_df\n",
    "    train_df = train_df.join(city_medians, on=\"city\", how=\"left\")\n",
    "\n",
    "    # Add median_city column to val_df, fallback to global median if city not in train\n",
    "    val_df = val_df.join(city_medians, on=\"city\", how=\"left\").withColumn(\n",
    "        \"median_city\",\n",
    "        F.when(F.col(\"median_city\").isNull(), F.lit(global_median)).otherwise(\n",
    "            F.col(\"median_city\")\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Optionally, save state for later use (e.g., as attributes)\n",
    "    fit_transform_city.city_medians_dict = city_medians_dict\n",
    "    fit_transform_city.city_centers_dict = city_centers_dict\n",
    "    fit_transform_city.global_median = global_median\n",
    "\n",
    "    return train_df, val_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13578df",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f041ffd0",
   "metadata": {},
   "source": [
    "MAGIC %md\n",
    "MAGIC ## Advanced Geospatial Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81ce8ca",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41c28ee",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def transform_neighborhoods_pre_filter(train_df, val_df):\n",
    "    \"\"\"\n",
    "    Performs HDBSCAN clustering based on geographic coordinates BEFORE price filtering.\n",
    "\n",
    "    Key features:\n",
    "    - Includes IQR-based outlier filtering to remove obviously misplaced listings\n",
    "    - Does NOT require price_cleaned column (works with nulls)\n",
    "    - Does NOT compute cluster medians (done separately post-filter)\n",
    "    - Preserves all columns from input DataFrames\n",
    "\n",
    "    This allows clustering to leverage full geographic density before filtering out\n",
    "    listings with null/invalid prices.\n",
    "    \"\"\"\n",
    "    mcs = 10  # Minimum Cluster Size\n",
    "\n",
    "    from pyspark.sql.types import LongType, StringType, StructField, StructType\n",
    "\n",
    "    def cluster_city_group(city_pdf):\n",
    "        \"\"\"\n",
    "        Clusters geographic coordinates using HDBSCAN after removing coordinate outliers.\n",
    "\n",
    "        Args:\n",
    "            city_pdf: Pandas DataFrame for one city\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with cluster_id column added\n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        from hdbscan import HDBSCAN\n",
    "\n",
    "        # Initialize cluster_id column\n",
    "        city_pdf[\"cluster_id\"] = -1\n",
    "\n",
    "        # ========== GEOSPATIAL OUTLIER FILTERING ==========\n",
    "        def remove_coordinate_outliers_iqr(pdf, k=3.0):\n",
    "            \"\"\"\n",
    "            Removes geospatial outliers using IQR method on lat/long.\n",
    "\n",
    "            Args:\n",
    "                pdf: Pandas DataFrame with 'lat' and 'long' columns\n",
    "                k: IQR multiplier (3.0 = conservative, removes ~0.7% if normal)\n",
    "\n",
    "            Returns:\n",
    "                Filtered pandas DataFrame with outliers removed\n",
    "            \"\"\"\n",
    "            if len(pdf) < 10:\n",
    "                return pdf\n",
    "\n",
    "            valid_mask = pdf[\"lat\"].notna() & pdf[\"long\"].notna()\n",
    "\n",
    "            if valid_mask.sum() < 10:\n",
    "                return pdf\n",
    "\n",
    "            valid_coords = pdf[valid_mask]\n",
    "\n",
    "            # Calculate IQR for latitude\n",
    "            q1_lat, q3_lat = valid_coords[\"lat\"].quantile([0.25, 0.75])\n",
    "            iqr_lat = q3_lat - q1_lat\n",
    "            lat_lower = q1_lat - k * iqr_lat\n",
    "            lat_upper = q3_lat + k * iqr_lat\n",
    "\n",
    "            # Calculate IQR for longitude\n",
    "            q1_long, q3_long = valid_coords[\"long\"].quantile([0.25, 0.75])\n",
    "            iqr_long = q3_long - q1_long\n",
    "            long_lower = q1_long - k * iqr_long\n",
    "            long_upper = q3_long + k * iqr_long\n",
    "\n",
    "            # Filter: keep nulls OR valid coordinates within bounds\n",
    "            keep_mask = ~valid_mask | (\n",
    "                (pdf[\"lat\"] >= lat_lower)\n",
    "                & (pdf[\"lat\"] <= lat_upper)\n",
    "                & (pdf[\"long\"] >= long_lower)\n",
    "                & (pdf[\"long\"] <= long_upper)\n",
    "            )\n",
    "\n",
    "            return pdf[keep_mask]\n",
    "\n",
    "        # Apply outlier filtering BEFORE clustering\n",
    "        city_pdf = remove_coordinate_outliers_iqr(city_pdf, k=3.0)\n",
    "\n",
    "        # Filter for valid coordinates\n",
    "        valid_coords = city_pdf[city_pdf[\"lat\"].notna() & city_pdf[\"long\"].notna()]\n",
    "\n",
    "        if len(valid_coords) < mcs:\n",
    "            # Not enough valid points to cluster\n",
    "            return city_pdf\n",
    "\n",
    "        # ========== HDBSCAN CLUSTERING ==========\n",
    "        coords_radians = np.radians(valid_coords[[\"lat\", \"long\"]].values)\n",
    "        clusterer = HDBSCAN(\n",
    "            min_cluster_size=mcs,\n",
    "            metric=\"haversine\",\n",
    "            cluster_selection_epsilon=0.0000005,\n",
    "        )\n",
    "        labels = clusterer.fit_predict(coords_radians)\n",
    "        city_pdf.loc[valid_coords.index, \"cluster_id\"] = labels\n",
    "\n",
    "        return city_pdf\n",
    "\n",
    "    # Apply clustering to training data\n",
    "    train_df = train_df.repartition(\"city\")\n",
    "\n",
    "    # Build output schema: existing fields + cluster_id\n",
    "    output_schema = StructType(\n",
    "        list(train_df.schema.fields) + [StructField(\"cluster_id\", LongType(), True)]\n",
    "    )\n",
    "\n",
    "    train_clustered = train_df.groupby(\"city\").applyInPandas(\n",
    "        cluster_city_group, schema=output_schema\n",
    "    )\n",
    "\n",
    "    # Collect train cluster mappings for validation assignment\n",
    "    train_clusters = (\n",
    "        train_clustered.filter(F.col(\"cluster_id\") != -1)\n",
    "        .select(\"city\", \"lat\", \"long\", \"cluster_id\")\n",
    "        .toPandas()\n",
    "    )\n",
    "\n",
    "    # Broadcast train clusters for efficient joining\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "    spark = SparkSession.getActiveSession()\n",
    "\n",
    "    if len(train_clusters) > 0:\n",
    "        # Create a UDF to assign validation points to nearest train cluster\n",
    "        def assign_val_clusters(val_pdf):\n",
    "            \"\"\"Assigns validation points to clusters using KNN on training data.\"\"\"\n",
    "            import numpy as np\n",
    "            import pandas as pd\n",
    "\n",
    "            val_pdf[\"cluster_id\"] = -1\n",
    "\n",
    "            for city in val_pdf[\"city\"].unique():\n",
    "                city_val = val_pdf[val_pdf[\"city\"] == city]\n",
    "                city_train = train_clusters[train_clusters[\"city\"] == city]\n",
    "\n",
    "                if len(city_train) == 0:\n",
    "                    continue\n",
    "\n",
    "                val_valid = city_val[city_val[\"lat\"].notna() & city_val[\"long\"].notna()]\n",
    "\n",
    "                if len(val_valid) == 0 or len(city_train) < 10:\n",
    "                    continue\n",
    "\n",
    "                from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "                knn = KNeighborsClassifier(\n",
    "                    n_neighbors=min(10, len(city_train)), metric=\"haversine\"\n",
    "                )\n",
    "                knn.fit(\n",
    "                    np.radians(city_train[[\"lat\", \"long\"]].values),\n",
    "                    city_train[\"cluster_id\"].astype(int),\n",
    "                )\n",
    "                preds = knn.predict(np.radians(val_valid[[\"lat\", \"long\"]].values))\n",
    "                val_pdf.loc[val_valid.index, \"cluster_id\"] = preds\n",
    "\n",
    "            return val_pdf\n",
    "\n",
    "        val_df = val_df.repartition(\"city\")\n",
    "        val_clustered = val_df.groupby(\"city\").applyInPandas(\n",
    "            assign_val_clusters, schema=output_schema\n",
    "        )\n",
    "    else:\n",
    "        # No valid clusters in training data, assign all to -1\n",
    "        val_clustered = val_df.withColumn(\"cluster_id\", F.lit(-1).cast(LongType()))\n",
    "\n",
    "    return train_clustered, val_clustered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b528535",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8965e2a4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def compute_cluster_medians(train_df, val_df):\n",
    "    \"\"\"\n",
    "    Computes median price per cluster using filtered training data.\n",
    "    Joins cluster medians to both train and validation sets.\n",
    "\n",
    "    Should be called AFTER price filtering (filter_valid_prices) to ensure\n",
    "    cluster medians are calculated from valid price data only.\n",
    "\n",
    "    Fallback hierarchy for cluster_median:\n",
    "    1. Cluster-specific median (from HDBSCAN clustering)\n",
    "    2. City-wide median (median_city column)\n",
    "    3. Global median (dataset-wide median)\n",
    "\n",
    "    Args:\n",
    "        train_df: Training DataFrame with cluster_id, price_cleaned, and median_city columns\n",
    "        val_df: Validation DataFrame with cluster_id and median_city columns\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (train_df, val_df) with cluster_median column added\n",
    "    \"\"\"\n",
    "    # Compute median price per (city, cluster_id) from training data\n",
    "    # Exclude noise points (cluster_id == -1) from median calculation\n",
    "    cluster_medians = (\n",
    "        train_df.filter(F.col(\"cluster_id\") != -1)  # Exclude noise points\n",
    "        .groupBy(\"city\", \"cluster_id\")\n",
    "        .agg(F.percentile_approx(\"price_cleaned\", 0.5).alias(\"cluster_median\"))\n",
    "    )\n",
    "\n",
    "    # Compute global median as final fallback for unseen clusters or noise\n",
    "    global_median = train_df.agg(F.percentile_approx(\"price_cleaned\", 0.5)).first()[0]\n",
    "\n",
    "    # Join to training data\n",
    "    train_df = train_df.join(cluster_medians, on=[\"city\", \"cluster_id\"], how=\"left\")\n",
    "\n",
    "    # Fill nulls (noise points or small clusters) with city_median, then global median\n",
    "    # Fallback hierarchy: cluster_median -> city_median -> global_median\n",
    "    train_df = train_df.withColumn(\n",
    "        \"cluster_median\",\n",
    "        F.when(\n",
    "            F.col(\"cluster_median\").isNull(),\n",
    "            F.coalesce(F.col(\"median_city\"), F.lit(global_median)),\n",
    "        ).otherwise(F.col(\"cluster_median\")),\n",
    "    )\n",
    "\n",
    "    # Join to validation data with same fallback hierarchy\n",
    "    val_df = val_df.join(\n",
    "        cluster_medians, on=[\"city\", \"cluster_id\"], how=\"left\"\n",
    "    ).withColumn(\n",
    "        \"cluster_median\",\n",
    "        F.when(\n",
    "            F.col(\"cluster_median\").isNull(),\n",
    "            F.coalesce(F.col(\"median_city\"), F.lit(global_median)),\n",
    "        ).otherwise(F.col(\"cluster_median\")),\n",
    "    )\n",
    "\n",
    "    return train_df, val_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c144c8a",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f8122b",
   "metadata": {},
   "source": [
    "MAGIC %md\n",
    "MAGIC ## ML Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d3c6d1",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e252ad",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Prepare Feature Vector\n",
    "def fit_transform_features(train_df, val_df, features=None):\n",
    "    if features is None:\n",
    "        features = train_df.columns\n",
    "    # Separate features by type for appropriate scaling\n",
    "    # Continuous features: will be imputed and scaled\n",
    "    continuous_features = list(\n",
    "        set(\n",
    "            [\n",
    "                # Raw property features\n",
    "                \"ratings\",\n",
    "                \"guests\",\n",
    "                \"property_number_of_reviews\",\n",
    "                \"num_beds\",\n",
    "                \"num_bedrooms\",\n",
    "                \"num_baths\",\n",
    "                # Raw host features\n",
    "                \"host_number_of_reviews\",\n",
    "                \"host_rating\",\n",
    "                \"host_year\",\n",
    "                # Geospatial features\n",
    "                \"cluster_median\",\n",
    "                # Interaction features\n",
    "                \"beds_per_guest\",\n",
    "                \"bedrooms_per_guest\",\n",
    "                \"guest_capacity_ratio\",\n",
    "                \"superhost_rating_interaction\",\n",
    "                \"review_volume_quality\",\n",
    "                \"total_rooms\",\n",
    "                \"bed_to_bedroom_ratio\",\n",
    "                \"rooms_per_guest\",\n",
    "                # Text features\n",
    "                \"amenities_count\",\n",
    "            ]\n",
    "        )\n",
    "        & set(features)\n",
    "    )\n",
    "\n",
    "    # Binary features: will be imputed but NOT scaled\n",
    "    # We dynamically include 'type_' columns created by transform_name\n",
    "    binary_type_cols = [c for c in features if c.startswith(\"type_\")]\n",
    "    binary_features = list(\n",
    "        set([\"is_superhost_binary\", \"is_studio_binary\"] + binary_type_cols)\n",
    "        & set(features)\n",
    "    )\n",
    "\n",
    "    # Step 1: Filter out features that are entirely null\n",
    "    # Check which continuous features have at least some non-null values\n",
    "    valid_continuous_features = []\n",
    "    for feature in continuous_features:\n",
    "        if feature in train_df.columns:\n",
    "            non_null_count = train_df.filter(F.col(feature).isNotNull()).count()\n",
    "            if non_null_count > 0:\n",
    "                valid_continuous_features.append(feature)\n",
    "            else:\n",
    "                print(\n",
    "                    f\"Warning: Feature '{feature}' has all null values and will be excluded\"\n",
    "                )\n",
    "\n",
    "    # Step 2: Impute continuous features\n",
    "    continuous_imputed_cols = [f\"{c}_imputed\" for c in valid_continuous_features]\n",
    "    imputer_continuous = Imputer(\n",
    "        inputCols=valid_continuous_features, outputCols=continuous_imputed_cols\n",
    "    ).setStrategy(\"median\")\n",
    "\n",
    "    imputer_model = imputer_continuous.fit(train_df)\n",
    "    train_df = imputer_model.transform(train_df)\n",
    "    val_df = imputer_model.transform(val_df)\n",
    "\n",
    "    # Step 3: Impute binary features\n",
    "    binary_imputed_cols = [f\"{c}_imputed\" for c in binary_features]\n",
    "    imputer_binary = Imputer(\n",
    "        inputCols=binary_features, outputCols=binary_imputed_cols\n",
    "    ).setStrategy(\"mode\")  # Use mode for binary\n",
    "\n",
    "    imputer_binary_model = imputer_binary.fit(train_df)\n",
    "    train_df = imputer_binary_model.transform(train_df)\n",
    "    val_df = imputer_binary_model.transform(val_df)\n",
    "\n",
    "    # Step 4: Assemble continuous features and scale them\n",
    "    assembler_continuous = VectorAssembler(\n",
    "        inputCols=continuous_imputed_cols,\n",
    "        outputCol=\"features_continuous_raw\",\n",
    "        handleInvalid=\"skip\",\n",
    "    )\n",
    "    train_df = assembler_continuous.transform(train_df)\n",
    "    val_df = assembler_continuous.transform(val_df)\n",
    "\n",
    "    # Scale continuous features with mean centering\n",
    "    scaler_continuous = StandardScaler(\n",
    "        inputCol=\"features_continuous_raw\",\n",
    "        outputCol=\"features_continuous_scaled\",\n",
    "        withStd=True,\n",
    "        withMean=True,  # Safe for dense continuous features\n",
    "    )\n",
    "    scaler_model = scaler_continuous.fit(train_df)\n",
    "    train_df = scaler_model.transform(train_df)\n",
    "    val_df = scaler_model.transform(val_df)\n",
    "\n",
    "    # Step 5: Assemble binary features (no scaling)\n",
    "    assembler_binary = VectorAssembler(\n",
    "        inputCols=binary_imputed_cols, outputCol=\"features_binary\", handleInvalid=\"skip\"\n",
    "    )\n",
    "    train_df = assembler_binary.transform(train_df)\n",
    "    val_df = assembler_binary.transform(val_df)\n",
    "\n",
    "    # Step 6: Combine all feature vectors (scaled continuous + unscaled binary + unscaled sparse)\n",
    "    assembler_final = VectorAssembler(\n",
    "        inputCols=[\"features_continuous_scaled\", \"features_binary\"],\n",
    "        outputCol=\"features\",\n",
    "        handleInvalid=\"skip\",\n",
    "    )\n",
    "    train_df = assembler_final.transform(train_df)\n",
    "    val_df = assembler_final.transform(val_df)\n",
    "\n",
    "    # Clean up intermediate columns (optional - keeps dataframe cleaner)\n",
    "    columns_to_drop = (\n",
    "        continuous_imputed_cols\n",
    "        + binary_imputed_cols\n",
    "        + [\"features_continuous_raw\", \"features_continuous_scaled\", \"features_binary\"]\n",
    "    )\n",
    "    train_df = train_df.drop(*columns_to_drop)\n",
    "    val_df = val_df.drop(*columns_to_drop)\n",
    "\n",
    "    # Return fitted components for model saving\n",
    "    return (\n",
    "        train_df,\n",
    "        val_df,\n",
    "        imputer_model,  # Continuous imputer\n",
    "        imputer_binary_model,  # Binary imputer\n",
    "        scaler_model,  # Scaler\n",
    "        assembler_continuous,  # Continuous assembler\n",
    "        assembler_binary,  # Binary assembler\n",
    "        assembler_final,  # Final assembler\n",
    "        valid_continuous_features,  # Feature names\n",
    "        binary_features,  # Feature names\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304b2653",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29924a22",
   "metadata": {},
   "source": [
    "MAGIC %md\n",
    "MAGIC ## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d95531c",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d85b48",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_models(train_data, val_data):\n",
    "    # ... (Keep your existing Model Dictionary and setup) ...\n",
    "    models = {\n",
    "        \"GBT_Deep_Slow\": GBTRegressor(\n",
    "            featuresCol=\"features\",\n",
    "            labelCol=\"price_cleaned\",\n",
    "            maxIter=40,\n",
    "            maxDepth=7,\n",
    "            stepSize=0.05,\n",
    "        )\n",
    "    }\n",
    "\n",
    "    n = val_data.count()\n",
    "    p = len(val_data.select(\"features\").first()[0])\n",
    "    results = []\n",
    "\n",
    "    for name, regressor in models.items():\n",
    "        try:\n",
    "            print(f\"Training {name}...\")\n",
    "            start_time = time.time()\n",
    "            model = regressor.fit(train_data)\n",
    "            predictions = model.transform(val_data)\n",
    "\n",
    "            predictions = predictions.withColumn(\n",
    "                \"prediction_real\", F.expm1(F.col(\"prediction\"))\n",
    "            )\n",
    "\n",
    "            # 1. VISUALIZATION LOGIC\n",
    "            # Optimized: Reduced sample size from 5000 to 1000 for faster visualization\n",
    "            plot_df = (\n",
    "                predictions.select(\"price_cleaned\", \"prediction\")\n",
    "                .sample(False, 0.1, seed=42)  # Adjust 0.1 based on data size\n",
    "                .limit(1000)  # Reduced from 5000 for faster plotting\n",
    "                .toPandas()\n",
    "            )\n",
    "\n",
    "            # Create single plot: Log-Log Scale\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "            ax.scatter(\n",
    "                plot_df[\"price_cleaned\"], plot_df[\"prediction\"], alpha=0.4, color=\"teal\"\n",
    "            )\n",
    "            min_val_log = min(\n",
    "                plot_df[\"price_cleaned\"].min(), plot_df[\"prediction\"].min()\n",
    "            )\n",
    "            max_val_log = max(\n",
    "                plot_df[\"price_cleaned\"].max(), plot_df[\"prediction\"].max()\n",
    "            )\n",
    "            ax.plot(\n",
    "                [min_val_log, max_val_log],\n",
    "                [min_val_log, max_val_log],\n",
    "                color=\"red\",\n",
    "                linestyle=\"--\",\n",
    "                label=\"Perfect Fit\",\n",
    "            )\n",
    "            ax.set_title(f\"Log-Log Prediction vs Reality: {name}\")\n",
    "            ax.set_xlabel(\"Actual Log(Price)\")\n",
    "            ax.set_ylabel(\"Predicted Log(Price)\")\n",
    "            ax.legend()\n",
    "            ax.grid(True, linestyle=\":\", alpha=0.6)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(\"results.png\")\n",
    "            # plt.show() # Do not show the plot\n",
    "\n",
    "            evaluator = RegressionEvaluator(\n",
    "                labelCol=\"price_cleaned\", predictionCol=\"prediction\"\n",
    "            )\n",
    "\n",
    "            rmse = evaluator.setMetricName(\"rmse\").evaluate(predictions)\n",
    "            r2 = evaluator.setMetricName(\"r2\").evaluate(predictions)\n",
    "            mae = evaluator.setMetricName(\"mae\").evaluate(predictions)\n",
    "\n",
    "            # Adjusted R2\n",
    "            adj_r2 = 1 - (1 - r2) * ((n - 1) / (n - p - 1))\n",
    "\n",
    "            # Manual MAP and MedAE (approximate or placeholder if too complex for pyspark SQL quickly)\n",
    "            # For debugging purposes, RMSE and R2 are sufficient to verify the fix.\n",
    "            medae = 0\n",
    "            mape = 0\n",
    "\n",
    "            end_time = time.time()\n",
    "            duration = end_time - start_time\n",
    "\n",
    "            # Calculate Training Metrics\n",
    "            train_predictions = model.transform(train_data)\n",
    "            train_rmse = evaluator.setMetricName(\"rmse\").evaluate(train_predictions)\n",
    "            train_r2 = evaluator.setMetricName(\"r2\").evaluate(train_predictions)\n",
    "            train_mae = evaluator.setMetricName(\"mae\").evaluate(train_predictions)\n",
    "\n",
    "            print(\n",
    "                f\"Result for {name}: Val RMSE={rmse:.4f}, Val R2={r2:.4f} | Train RMSE={train_rmse:.4f}, Train R2={train_r2:.4f}\"\n",
    "            )\n",
    "\n",
    "            results.append(\n",
    "                {\n",
    "                    \"Model\": name,\n",
    "                    \"R2\": r2,\n",
    "                    \"Adj_R2\": adj_r2,\n",
    "                    \"RMSE\": rmse,\n",
    "                    \"MAE\": mae,\n",
    "                    \"MedAE\": medae,\n",
    "                    \"MAPE\": mape * 100,\n",
    "                    \"Train_R2\": train_r2,\n",
    "                    \"Train_RMSE\": train_rmse,\n",
    "                    \"Train_MAE\": train_mae,\n",
    "                    \"Time\": duration,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {name} due to error: {e}\")\n",
    "            model = None\n",
    "\n",
    "    # Return results and the trained model\n",
    "    return results, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dad794",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a152b8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_CATEGORIES = {\n",
    "    # Raw Property Features\n",
    "    \"guests\": \"Raw Property\",\n",
    "    \"num_beds\": \"Raw Property\",\n",
    "    \"num_bedrooms\": \"Raw Property\",\n",
    "    \"num_baths\": \"Raw Property\",\n",
    "    \"ratings\": \"Raw Property\",\n",
    "    \"property_number_of_reviews\": \"Raw Property\",\n",
    "    # Raw Host Features\n",
    "    \"host_rating\": \"Raw Host\",\n",
    "    \"host_year\": \"Raw Host\",\n",
    "    \"host_number_of_reviews\": \"Raw Host\",\n",
    "    # Geospatial Features\n",
    "    \"cluster_median\": \"Geospatial\",\n",
    "    # Engineered Features\n",
    "    \"beds_per_guest\": \"Engineered\",\n",
    "    \"bedrooms_per_guest\": \"Engineered\",\n",
    "    \"guest_capacity_ratio\": \"Engineered\",\n",
    "    \"superhost_rating_interaction\": \"Engineered\",\n",
    "    \"review_volume_quality\": \"Engineered\",\n",
    "    \"total_rooms\": \"Engineered\",\n",
    "    \"bed_to_bedroom_ratio\": \"Engineered\",\n",
    "    \"rooms_per_guest\": \"Engineered\",\n",
    "    # Text Features\n",
    "    \"amenities_count\": \"Text Features\",\n",
    "    # Binary Features\n",
    "    \"is_superhost_binary\": \"Binary\",\n",
    "    \"is_studio_binary\": \"Binary\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1944591",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e16bf16",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "CATEGORY_COLORS = {\n",
    "    \"Raw Property\": \"#2ecc71\",  # Green\n",
    "    \"Raw Host\": \"#e67e22\",  # Orange\n",
    "    \"Geospatial\": \"#3498db\",  # Blue\n",
    "    \"Engineered\": \"#e74c3c\",  # Red\n",
    "    \"Text Features\": \"#9b59b6\",  # Purple\n",
    "    \"Binary\": \"#95a5a6\",  # Gray\n",
    "    \"Other\": \"#34495e\",  # Dark gray\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84d6826",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c04c5e7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def categorize_feature(feature_name):\n",
    "    \"\"\"\n",
    "    Map a feature name to its category.\n",
    "    Handles '_imputed' suffix and 'type_*' binary columns.\n",
    "    \"\"\"\n",
    "    clean_name = feature_name.replace(\"_imputed\", \"\")\n",
    "\n",
    "    # Check if it's a type column (binary)\n",
    "    if clean_name.startswith(\"type_\"):\n",
    "        return \"Binary\"\n",
    "\n",
    "    # Look up in category dictionary\n",
    "    return FEATURE_CATEGORIES.get(clean_name, \"Other\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb15005a",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12145eae",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def plot_feature_importance(\n",
    "    gbt_model,\n",
    "    continuous_features,\n",
    "    binary_features,\n",
    "    output_file=\"feature_importance.png\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Extract feature importances from trained GBT model and create a horizontal bar chart.\n",
    "    Features are color-coded by category.\n",
    "    \"\"\"\n",
    "    # Extract feature importances vector\n",
    "    importances = gbt_model.featureImportances\n",
    "\n",
    "    # Build feature name list (must match order: continuous first, then binary)\n",
    "    feature_names = [f\"{f}_imputed\" for f in continuous_features] + [\n",
    "        f\"{f}_imputed\" for f in binary_features\n",
    "    ]\n",
    "\n",
    "    # Create DataFrame\n",
    "    importance_df = pd.DataFrame(\n",
    "        {\n",
    "            \"feature\": feature_names,\n",
    "            \"importance\": [importances[i] for i in range(len(feature_names))],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Add category and color\n",
    "    importance_df[\"category\"] = importance_df[\"feature\"].apply(categorize_feature)\n",
    "    importance_df[\"color\"] = importance_df[\"category\"].map(CATEGORY_COLORS)\n",
    "\n",
    "    # Sort by importance\n",
    "    importance_df = importance_df.sort_values(\"importance\", ascending=True)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, max(8, len(feature_names) * 0.3)))\n",
    "    bars = plt.barh(\n",
    "        range(len(importance_df)),\n",
    "        importance_df[\"importance\"],\n",
    "        color=importance_df[\"color\"],\n",
    "        alpha=0.8,\n",
    "    )\n",
    "\n",
    "    plt.yticks(\n",
    "        range(len(importance_df)),\n",
    "        [f.replace(\"_imputed\", \"\") for f in importance_df[\"feature\"]],\n",
    "    )\n",
    "    plt.xlabel(\"Importance Score\", fontsize=12)\n",
    "    plt.title(\"Feature Importance (GBT Model)\", fontsize=14, fontweight=\"bold\")\n",
    "    plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "    # Add legend\n",
    "    from matplotlib.patches import Patch\n",
    "\n",
    "    legend_elements = [\n",
    "        Patch(facecolor=CATEGORY_COLORS[cat], label=cat, alpha=0.8)\n",
    "        for cat in importance_df[\"category\"].unique()\n",
    "    ]\n",
    "    plt.legend(handles=legend_elements, loc=\"lower right\", fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    return importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea057283",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6557d3d2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def plot_feature_correlations(\n",
    "    train_df,\n",
    "    continuous_features,\n",
    "    output_file=\"feature_correlation.png\",\n",
    "    sample_size=10000,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute Pearson correlation for continuous features and plot heatmap.\n",
    "    \"\"\"\n",
    "    # Sample data efficiently\n",
    "    total_count = train_df.count()\n",
    "    fraction = min(1.0, sample_size / total_count)\n",
    "\n",
    "    # Select only continuous features (check which columns exist in the DataFrame)\n",
    "    available_features = []\n",
    "    df_columns = set(train_df.columns)\n",
    "\n",
    "    # Try with _imputed suffix first, if not available try without\n",
    "    for f in continuous_features:\n",
    "        if f\"{f}_imputed\" in df_columns:\n",
    "            available_features.append(f\"{f}_imputed\")\n",
    "        elif f in df_columns:\n",
    "            available_features.append(f)\n",
    "\n",
    "    if not available_features:\n",
    "        print(\n",
    "            \"   Warning: No continuous features found in DataFrame, skipping correlation\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    sampled_df = (\n",
    "        train_df.select(available_features)\n",
    "        .sample(withReplacement=False, fraction=fraction, seed=42)\n",
    "        .toPandas()\n",
    "    )\n",
    "\n",
    "    # Compute correlation matrix\n",
    "    corr_matrix = sampled_df.corr()\n",
    "\n",
    "    # Clean feature names for display\n",
    "    corr_matrix.index = [f.replace(\"_imputed\", \"\") for f in corr_matrix.index]\n",
    "    corr_matrix.columns = [f.replace(\"_imputed\", \"\") for f in corr_matrix.columns]\n",
    "\n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)  # Mask upper triangle\n",
    "\n",
    "    sns.heatmap(\n",
    "        corr_matrix,\n",
    "        mask=mask,\n",
    "        cmap=\"RdBu_r\",\n",
    "        center=0,\n",
    "        vmin=-1,\n",
    "        vmax=1,\n",
    "        annot=False,\n",
    "        fmt=\".2f\",\n",
    "        square=True,\n",
    "        linewidths=0.5,\n",
    "        cbar_kws={\"shrink\": 0.8},\n",
    "    )\n",
    "\n",
    "    # Annotate only strong correlations\n",
    "    for i in range(len(corr_matrix)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > 0.5:\n",
    "                plt.text(\n",
    "                    j + 0.5,\n",
    "                    i + 0.5,\n",
    "                    f\"{corr_matrix.iloc[i, j]:.2f}\",\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    fontsize=8,\n",
    "                    fontweight=\"bold\",\n",
    "                )\n",
    "\n",
    "    plt.title(\n",
    "        f\"Feature Correlation Heatmap (n={len(sampled_df):,})\",\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "        pad=20,\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f9ab9c",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bdf03d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def plot_feature_distributions(\n",
    "    train_df, importance_df, output_file=\"feature_distributions.png\", sample_size=15000\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot distribution of top 9 features by importance.\n",
    "    Each subplot shows histogram + box plot + summary statistics.\n",
    "    \"\"\"\n",
    "    # Get top 9 features\n",
    "    top_9_features = importance_df.nlargest(9, \"importance\")[\"feature\"].tolist()\n",
    "\n",
    "    # Find which columns actually exist in the DataFrame\n",
    "    df_columns = set(train_df.columns)\n",
    "    available_features = []\n",
    "    for f in top_9_features:\n",
    "        # Try _imputed suffix first, then without\n",
    "        if f in df_columns:\n",
    "            available_features.append(f)\n",
    "        else:\n",
    "            # Try without _imputed suffix\n",
    "            f_clean = f.replace(\"_imputed\", \"\")\n",
    "            if f_clean in df_columns:\n",
    "                available_features.append(f_clean)\n",
    "\n",
    "    if not available_features:\n",
    "        print(\"   Warning: No features found in DataFrame for distributions, skipping\")\n",
    "        return\n",
    "\n",
    "    # Take only top 9 available\n",
    "    top_9 = available_features[:9]\n",
    "\n",
    "    # Sample data\n",
    "    total_count = train_df.count()\n",
    "    fraction = min(1.0, sample_size / total_count)\n",
    "    sampled_df = (\n",
    "        train_df.select(top_9)\n",
    "        .sample(withReplacement=False, fraction=fraction, seed=42)\n",
    "        .toPandas()\n",
    "    )\n",
    "\n",
    "    # Create 3x3 subplot grid\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(16, 12))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, feature in enumerate(top_9):\n",
    "        ax = axes[idx]\n",
    "        data = sampled_df[feature].dropna()\n",
    "\n",
    "        # Histogram\n",
    "        ax.hist(data, bins=30, alpha=0.7, color=\"steelblue\", edgecolor=\"black\")\n",
    "\n",
    "        # Add box plot overlay (at top)\n",
    "        ax2 = ax.twiny()\n",
    "        ax2.boxplot(\n",
    "            [data],\n",
    "            vert=False,\n",
    "            positions=[ax.get_ylim()[1] * 0.9],\n",
    "            widths=ax.get_ylim()[1] * 0.1,\n",
    "            patch_artist=True,\n",
    "            boxprops=dict(facecolor=\"orange\", alpha=0.6),\n",
    "        )\n",
    "        ax2.set_xlim(ax.get_xlim())\n",
    "        ax2.set_xticks([])\n",
    "\n",
    "        # Statistics\n",
    "        mean_val = data.mean()\n",
    "        median_val = data.median()\n",
    "        std_val = data.std()\n",
    "        skew_val = stats.skew(data)\n",
    "\n",
    "        # Clean feature name\n",
    "        clean_name = feature.replace(\"_imputed\", \"\")\n",
    "\n",
    "        ax.set_title(\n",
    "            f\"{clean_name}\\n={mean_val:.2f}, ={std_val:.2f}, skew={skew_val:.2f}\",\n",
    "            fontsize=10,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "        ax.set_xlabel(\"Value\", fontsize=9)\n",
    "        ax.set_ylabel(\"Frequency\", fontsize=9)\n",
    "        ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "    plt.suptitle(\n",
    "        f\"Feature Distributions (Top 9 by Importance, n={len(sampled_df):,})\",\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "        y=0.995,\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ab0002",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a893d65",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def plot_feature_impact(\n",
    "    val_df, importance_df, output_file=\"feature_impact.png\", sample_size=5000\n",
    "):\n",
    "    \"\"\"\n",
    "    Partial dependence plots for top 6 features.\n",
    "    Shows how changing feature values affects predicted price.\n",
    "    \"\"\"\n",
    "    # Get top 6 features\n",
    "    top_6_features = importance_df.nlargest(6, \"importance\")[\"feature\"].tolist()\n",
    "\n",
    "    # Find which columns actually exist in the DataFrame\n",
    "    df_columns = set(val_df.columns)\n",
    "    available_features = []\n",
    "    for f in top_6_features:\n",
    "        # Try with the feature name as-is first, then without _imputed suffix\n",
    "        if f in df_columns:\n",
    "            available_features.append(f)\n",
    "        else:\n",
    "            f_clean = f.replace(\"_imputed\", \"\")\n",
    "            if f_clean in df_columns:\n",
    "                available_features.append(f_clean)\n",
    "\n",
    "    if not available_features:\n",
    "        print(\n",
    "            \"   Warning: No features found in DataFrame for impact analysis, skipping\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    # Take only top 6 available\n",
    "    top_6 = available_features[:6]\n",
    "\n",
    "    # Sample validation predictions\n",
    "    total_count = val_df.count()\n",
    "    fraction = min(1.0, sample_size / total_count)\n",
    "    sampled_df = (\n",
    "        val_df.select(top_6 + [\"prediction\"])\n",
    "        .sample(withReplacement=False, fraction=fraction, seed=42)\n",
    "        .toPandas()\n",
    "    )\n",
    "\n",
    "    # Create 2x3 subplot grid\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, feature in enumerate(top_6):\n",
    "        ax = axes[idx]\n",
    "        data = sampled_df[[feature, \"prediction\"]].dropna()\n",
    "\n",
    "        # Bin feature values into 20 bins\n",
    "        data[\"feature_binned\"] = pd.cut(data[feature], bins=20)\n",
    "\n",
    "        # Calculate mean prediction and std dev per bin\n",
    "        grouped = data.groupby(\"feature_binned\")[\"prediction\"].agg(\n",
    "            [\"mean\", \"std\", \"count\"]\n",
    "        )\n",
    "        grouped = grouped[grouped[\"count\"] >= 5]  # Only bins with 5+ samples\n",
    "\n",
    "        # Get bin centers for x-axis\n",
    "        bin_centers = [interval.mid for interval in grouped.index]\n",
    "\n",
    "        # Plot mean prediction\n",
    "        ax.plot(bin_centers, grouped[\"mean\"], color=\"darkblue\", linewidth=2, marker=\"o\")\n",
    "\n",
    "        # Add confidence bands (1 std dev)\n",
    "        ax.fill_between(\n",
    "            bin_centers,\n",
    "            grouped[\"mean\"] - grouped[\"std\"],\n",
    "            grouped[\"mean\"] + grouped[\"std\"],\n",
    "            alpha=0.3,\n",
    "            color=\"lightblue\",\n",
    "            label=\"1 \",\n",
    "        )\n",
    "\n",
    "        # Clean feature name\n",
    "        clean_name = feature.replace(\"_imputed\", \"\")\n",
    "\n",
    "        ax.set_title(f\"Impact of {clean_name} on Price\", fontsize=11, fontweight=\"bold\")\n",
    "        ax.set_xlabel(clean_name, fontsize=10)\n",
    "        ax.set_ylabel(\"Predicted Log(Price)\", fontsize=10)\n",
    "        ax.grid(linestyle=\"--\", alpha=0.4)\n",
    "        ax.legend(loc=\"best\", fontsize=8)\n",
    "\n",
    "    plt.suptitle(\n",
    "        f\"Feature Impact on Price Prediction (n={len(sampled_df):,})\",\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "        y=0.995,\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e94e89d",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9796105c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def plot_engineering_impact(\n",
    "    importance_df, output_file=\"feature_engineering_impact.png\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare total importance by feature category.\n",
    "    Shows side-by-side: grouped bar chart + pie chart.\n",
    "    \"\"\"\n",
    "    # Group by category and sum importances\n",
    "    category_importance = (\n",
    "        importance_df.groupby(\"category\")[\"importance\"]\n",
    "        .sum()\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "\n",
    "    # Create side-by-side plots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    # Bar chart\n",
    "    colors = [CATEGORY_COLORS[cat] for cat in category_importance.index]\n",
    "    bars = ax1.bar(\n",
    "        range(len(category_importance)),\n",
    "        category_importance.values,\n",
    "        color=colors,\n",
    "        alpha=0.8,\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "    ax1.set_xticks(range(len(category_importance)))\n",
    "    ax1.set_xticklabels(category_importance.index, rotation=30, ha=\"right\")\n",
    "    ax1.set_ylabel(\"Total Importance\", fontsize=12)\n",
    "    ax1.set_title(\n",
    "        \"Total Feature Importance by Category\", fontsize=13, fontweight=\"bold\"\n",
    "    )\n",
    "    ax1.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for i, (bar, val) in enumerate(zip(bars, category_importance.values)):\n",
    "        ax1.text(\n",
    "            bar.get_x() + bar.get_width() / 2,\n",
    "            bar.get_height() + 0.01,\n",
    "            f\"{val:.3f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=10,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "    # Pie chart\n",
    "    wedges, texts, autotexts = ax2.pie(\n",
    "        category_importance.values,\n",
    "        labels=category_importance.index,\n",
    "        colors=colors,\n",
    "        autopct=\"%1.1f%%\",\n",
    "        startangle=90,\n",
    "        textprops={\"fontsize\": 11},\n",
    "    )\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color(\"white\")\n",
    "        autotext.set_fontweight(\"bold\")\n",
    "    ax2.set_title(\"Feature Category Distribution\", fontsize=13, fontweight=\"bold\")\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"Feature Engineering ROI Analysis\", fontsize=15, fontweight=\"bold\", y=0.98\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d15958",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f861a7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def visualize_feature_selection(\n",
    "    train_df,\n",
    "    val_df,\n",
    "    gbt_model,\n",
    "    continuous_features,\n",
    "    binary_features,\n",
    "    output_dir=\"./artifacts/\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate all 5 feature selection visualizations.\n",
    "    Call this after model training is complete.\n",
    "    \"\"\"\n",
    "\n",
    "    importance_df = plot_feature_importance(\n",
    "        gbt_model,\n",
    "        continuous_features,\n",
    "        binary_features,\n",
    "        output_file=f\"{output_dir}/feature_importance.png\",\n",
    "    )\n",
    "    print(\"   Saved feature_importance.png\")\n",
    "\n",
    "    plot_feature_correlations(\n",
    "        train_df,\n",
    "        continuous_features,\n",
    "        output_file=f\"{output_dir}/feature_correlation.png\",\n",
    "        sample_size=10000,\n",
    "    )\n",
    "    print(\"   Saved feature_correlation.png\")\n",
    "\n",
    "    plot_feature_distributions(\n",
    "        train_df,\n",
    "        importance_df,\n",
    "        output_file=f\"{output_dir}/feature_distributions.png\",\n",
    "        sample_size=15000,\n",
    "    )\n",
    "    print(\"   Saved feature_distributions.png\")\n",
    "\n",
    "    plot_feature_impact(\n",
    "        val_df,\n",
    "        importance_df,\n",
    "        output_file=f\"{output_dir}/feature_impact.png\",\n",
    "        sample_size=5000,\n",
    "    )\n",
    "    # [5/5] Engineering ROI\n",
    "    print(\"\\n[5/5] Creating engineering ROI comparison...\")\n",
    "    plot_engineering_impact(\n",
    "        importance_df, output_file=f\"{output_dir}/feature_engineering_impact.png\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c7b1fd",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8f6242",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def visualize_city_clusters(spark_df, city_name=\"Greater London\", sample_size=20000):\n",
    "    \"\"\"\n",
    "    Filters the Spark DF by city, converts a sample to Pandas,\n",
    "    and plots the clusters.\n",
    "    \"\"\"\n",
    "    # 1. Filter and Sample in Spark (efficiently reduces data before collection)\n",
    "    city_data = spark_df.filter(F.col(\"city\") == city_name)\n",
    "\n",
    "    # Calculate fraction for sampling to hit roughly the sample_size\n",
    "    total_count = city_data.count()\n",
    "    if total_count == 0:\n",
    "        print(f\"No data found for city: {city_name}\")\n",
    "        return\n",
    "\n",
    "    fraction = min(1.0, sample_size / total_count)\n",
    "    pdf = city_data.sample(withReplacement=False, fraction=fraction, seed=42).toPandas()\n",
    "\n",
    "    # 2. Setup Plotting\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Separate noise and clustered points for better styling\n",
    "    noise = pdf[pdf[\"cluster_id\"] == -1]\n",
    "    clustered = pdf[pdf[\"cluster_id\"] != -1]\n",
    "\n",
    "    # Plot Noise (Light Gray, small points)\n",
    "    plt.scatter(\n",
    "        noise[\"long\"],\n",
    "        noise[\"lat\"],\n",
    "        c=\"lightgray\",\n",
    "        s=5,\n",
    "        label=\"Noise/Outliers\",\n",
    "        alpha=0.4,\n",
    "    )\n",
    "\n",
    "    # Plot Clusters (Using a colormap for distinct neighborhoods)\n",
    "    if not clustered.empty:\n",
    "        scatter = plt.scatter(\n",
    "            clustered[\"long\"],\n",
    "            clustered[\"lat\"],\n",
    "            c=clustered[\"cluster_id\"],\n",
    "            cmap=\"turbo\",\n",
    "            s=15,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "        # Add a colorbar to show cluster IDs\n",
    "        plt.colorbar(scatter, label=\"Cluster ID\")\n",
    "\n",
    "    plt.title(f\"HDBSCAN Neighborhood Clusters: {city_name.capitalize()}\", fontsize=15)\n",
    "    plt.xlabel(\"Longitude\")\n",
    "    plt.ylabel(\"Latitude\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "    # Create filename from city name (e.g., \"Greater London\" -> \"greater_london_clusters.png\")\n",
    "    filename = f\"{city_name.lower().replace(' ', '_')}_clusters.png\"\n",
    "    plt.savefig(filename)\n",
    "    print(f\"   Saved {filename}\")\n",
    "    plt.close()  # Close figure to free memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2a33f1",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e7ad68",
   "metadata": {},
   "source": [
    "MAGIC %md\n",
    "MAGIC ## Pipeline Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b55466e",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5251bfd1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def apply_stateless_transformations(df):\n",
    "    df = initial_selection(df)\n",
    "    df = set_schema(df)\n",
    "    df = prepare_price(df)  # CHANGED: Prepares price_cleaned but does NOT filter nulls\n",
    "    df = transform_details(df)\n",
    "    df = transform_location(df)\n",
    "    df = filter_top_k_cities(df)\n",
    "    df = transform_name(df)\n",
    "    df = transform_superhost(df)\n",
    "    df = transform_amenities(df)\n",
    "    df = create_interaction_features(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d883ec1",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72e7fd9",
   "metadata": {},
   "source": [
    "MAGIC %md\n",
    "MAGIC ## Running"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab6e302",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a37afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar = create_calendar(calendar)\n",
    "top_cities = [row[\"city\"] for row in calendar.select(\"city\").distinct().collect()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674d2d69",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe01a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply stateless transformations\n",
    "df = apply_stateless_transformations(airbnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e1b16e",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dd019f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache before split\n",
    "df = df.cache()\n",
    "df.count()  # Trigger cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fc516b",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c60aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/val\n",
    "train_df, val_df = df.randomSplit([0.85, 0.15], seed=42)\n",
    "# Unpersist parent DataFrame since children are now cached\n",
    "df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4aae12",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0418a605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering\n",
    "train_df, val_df = transform_neighborhoods_pre_filter(train_df, val_df)\n",
    "train_df = train_df.cache()\n",
    "train_df.count()\n",
    "val_df = val_df.cache()\n",
    "val_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eedcd7",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3555c2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_to_visualize = [\"Greater London\", \"Paris\", \"Austin\"]\n",
    "for city in cities_to_visualize:\n",
    "    visualize_city_clusters(train_df, city_name=city, sample_size=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776b7d62",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a91cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = filter_valid_prices(train_df)\n",
    "val_df = filter_valid_prices(val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78fbde1",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16111f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = fit_transform_city(train_df, val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85836373",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9562d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = compute_cluster_medians(train_df, val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a002bc",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d721283",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.cache()\n",
    "train_df.count()\n",
    "val_df = val_df.cache()\n",
    "val_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a32f2f",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72aba30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpack fitted components from fit_transform_features\n",
    "(\n",
    "    train_df,\n",
    "    val_df,\n",
    "    imputer_cont,\n",
    "    imputer_bin,\n",
    "    scaler,\n",
    "    asm_cont,\n",
    "    asm_bin,\n",
    "    asm_final,\n",
    "    cont_features,\n",
    "    bin_features,\n",
    ") = fit_transform_features(train_df, val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ea1ef2",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582ff3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results, gbt_model = train_models(train_df, val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b525da6",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d5412c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df_with_predictions = gbt_model.transform(val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a864aa",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01854bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_feature_selection(\n",
    "    train_df=train_df,\n",
    "    val_df=val_df_with_predictions,\n",
    "    gbt_model=gbt_model,\n",
    "    continuous_features=cont_features,\n",
    "    binary_features=bin_features,\n",
    "    output_dir=\"./visualizations\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba28bde",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da8a676",
   "metadata": {},
   "source": [
    "MAGIC %run ./save_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cb6575",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1129d477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare performance metrics\n",
    "performance_metrics = {\n",
    "    \"R2\": results[0][\"R2\"],\n",
    "    \"RMSE\": results[0][\"RMSE\"],\n",
    "    \"MAE\": results[0][\"MAE\"],\n",
    "    \"Train_R2\": results[0][\"Train_R2\"],\n",
    "    \"Train_RMSE\": results[0][\"Train_RMSE\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89070e89",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68697b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save everything\n",
    "save_pipeline_artifacts(\n",
    "    gbt_model=gbt_model,\n",
    "    imputer_continuous_model=imputer_cont,\n",
    "    imputer_binary_model=imputer_bin,\n",
    "    scaler_model=scaler,\n",
    "    assembler_continuous=asm_cont,\n",
    "    assembler_binary=asm_bin,\n",
    "    assembler_final=asm_final,\n",
    "    train_df=train_df,  # Pass DataFrame for Parquet save\n",
    "    city_medians_dict=fit_transform_city.city_medians_dict,\n",
    "    city_centers_dict=fit_transform_city.city_centers_dict,\n",
    "    global_median=fit_transform_city.global_median,\n",
    "    continuous_features=cont_features,\n",
    "    binary_features=bin_features,\n",
    "    performance_metrics=performance_metrics,\n",
    "    output_dir=\"./artifacts\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc63061",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e3be02",
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar.write.mode(\"overwrite\").parquet(\"./artifacts/calendar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2b0a07",
   "metadata": {},
   "source": [
    "COMMAND ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5eb2ac",
   "metadata": {},
   "source": [
    "MAGIC %md\n",
    "MAGIC ## Downloading data\n",
    "MAGIC\n",
    "MAGIC - In order to download the model and artifacts to the local machine, we will use databricks-cli\n",
    "MAGIC - The processed data weighs a total of 2.5MBs\n",
    "MAGIC - install the CLI by following the instructions: https://docs.databricks.com/aws/en/dev-tools/cli/install\n",
    "MAGIC - authenticate to gain access to the workspace\n",
    "MAGIC - download the parquet files using `databricks fs cp dbfs:/artifacts . --recursive`\n",
    "MAGIC - download the .json files in this workspace manually by clicking on 'artifacts' and 'download ZIP'\n",
    "MAGIC - Alternatively, the latest model is available on the projects github page https://github.com/RazEly/airbnb-predictor which requires no further setup, simply follow the README"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
